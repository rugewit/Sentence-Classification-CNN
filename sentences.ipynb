{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 02:05:08.375837: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 54321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified data/train_5500.label\n",
      "Found and verified data/TREC_10.label\n"
     ]
    }
   ],
   "source": [
    "url = 'http://cogcomp.org/Data/QA/QC/'\n",
    "dir_name = 'data'\n",
    "\n",
    "def download_data(dir_name, filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  \n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    if not os.path.exists(os.path.join(dir_name,filename)):\n",
    "        filepath, _ = urlretrieve(url + filename, os.path.join(dir_name,filename))\n",
    "    else:\n",
    "        filepath = os.path.join(dir_name, filename)\n",
    "    \n",
    "    statinfo = os.stat(filepath)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filepath)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filepath + '. Can you get to it with a browser?')\n",
    "        \n",
    "    return filepath\n",
    "\n",
    "train_filename = download_data(dir_name, 'train_5500.label', 335858)\n",
    "test_filename = download_data(dir_name, 'TREC_10.label',23354)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    questions, categories, sub_categories = [], [], []\n",
    "    with open(filename, 'r', encoding='latin-1') as f:\n",
    "        # read each line\n",
    "        for row in f:\n",
    "            row_str = row.split(':')\n",
    "            cat, sub_cat_and_question = row_str[0], row_str[1]\n",
    "            tokens = sub_cat_and_question.split(' ')\n",
    "            # The first word in sub_cat_and_question is the sub\n",
    "            # category rest is the question\n",
    "            sub_cat, question = tokens[0], ' '.join(tokens[1:])\n",
    "\n",
    "            questions.append(question.lower().strip())\n",
    "            categories.append(cat)\n",
    "            sub_categories.append(sub_cat)\n",
    "    return questions, categories, sub_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_questions, train_categories, train_sub_categories = read_data(train_filename)\n",
    "test_questions, test_categories, test_sub_categories = read_data(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training and testing\n",
    "train_df = pd.DataFrame(\n",
    "{'question': train_questions, 'category': train_categories,\n",
    "    'sub_category': train_sub_categories}\n",
    ")\n",
    "test_df = pd.DataFrame(\n",
    "{'question': test_questions, 'category': test_categories,\n",
    "    'sub_category': test_sub_categories}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data for better randomization\n",
    "train_df = train_df.sample(frac=1.0, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label->ID mapping: {'DESC': 0, 'ENTY': 1, 'LOC': 2, 'NUM': 3, 'HUM': 4, 'ABBR': 5}\n"
     ]
    }
   ],
   "source": [
    "unique_cats = train_df['category'].unique()\n",
    "# np.arange(unique_cats.shape[0] generates numbers from 0 to size of unique_cats\n",
    "labels_map = dict(zip(unique_cats, np.arange(unique_cats.shape[0])))\n",
    "print(f'Label->ID mapping: {labels_map}')\n",
    "\n",
    "n_classes = len(labels_map)\n",
    "# convert all string Labels to IDs\n",
    "train_df['category'] = train_df['category'].map(labels_map)\n",
    "test_df['category'] = test_df['category'].map(labels_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:(4906, 3)\n",
      "valid size:(546, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, valid_df = train_test_split(train_df, test_size=0.1)\n",
    "print(f'train size:{train_df.shape}')\n",
    "print(f'valid size:{valid_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df['question'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabluary size: 7917\n"
     ]
    }
   ],
   "source": [
    "n_vocab = len(tokenizer.index_word) + 1\n",
    "print(f\"Vocabluary size: {n_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_df['question'].tolist())\n",
    "train_labels = train_df['category'].values\n",
    "\n",
    "valid_sequences = tokenizer.texts_to_sequences(valid_df['question'].tolist())\n",
    "valid_labels = valid_df['category'].values\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['question'].tolist())\n",
    "test_labels = test_df['category'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "max_seq_length = 22\n",
    "\n",
    "preprocessed_res = partial(\n",
    "    tf.keras.preprocessing.sequence.pad_sequences,\n",
    "    maxlen=max_seq_length, padding='post', truncating='post')\n",
    "\n",
    "preprocessed_train_sequences = preprocessed_res(train_sequences)\n",
    "preprocessed_valid_sequences = preprocessed_res(valid_sequences)\n",
    "preprocessed_test_sequences = preprocessed_res(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 02:05:11.909873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-05 02:05:12.015486: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-05 02:05:12.015862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-05 02:05:12.017407: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-05 02:05:12.017667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-05 02:05:12.017902: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-05 02:05:12.875048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-05 02:05:12.875276: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-05 02:05:12.875445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-05 02:05:12.875583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1852 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:07:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Input layer takes word IDs as inputs\n",
    "word_id_inputs = layers.Input(shape=(max_seq_length,), dtype='int32')\n",
    "# Get the embeddings of the inputs / out [batch_size, sent_length,\n",
    "# output_dim]\n",
    "embedding_out = layers.Embedding(input_dim=n_vocab, output_dim=64)(word_id_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all layers: in [batch_size, sent_length, emb_size] / out [batch_\n",
    "# size, sent_length, 100]\n",
    "\n",
    "conv1_1 = layers.Conv1D(100, kernel_size=3, \n",
    "                        strides=1, padding='same', \n",
    "                        activation='relu')(embedding_out)\n",
    "conv1_2 = layers.Conv1D(100, kernel_size=4, \n",
    "                        strides=1, padding='same', \n",
    "                        activation='relu')(embedding_out)\n",
    "conv1_3 = layers.Conv1D(100, kernel_size=5, \n",
    "                        strides=1, padding='same', \n",
    "                        activation='relu')(embedding_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in previous conv outputs / out [batch_size, sent_length, 300]\n",
    "conv_out = layers.Concatenate(axis=-1)([conv1_1, conv1_2, conv1_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling over time operation.\n",
    "# This is doing the max pooling over sequence length\n",
    "# in other words, each feature map results in a single output\n",
    "# in [batch_size, sent_length, 300] / out [batch_size, 1, 300]\n",
    "\n",
    "pool_over_time_out = layers.MaxPool1D(pool_size=max_seq_length, \n",
    "                                      padding='valid')(conv_out)\n",
    "# imply collapses all the dimensions (except the batch dimension)\n",
    "# to a single dimension\n",
    "flatten_out = layers.Flatten()(pool_over_time_out)\n",
    "\n",
    "out = layers.Dense(n_classes, activation='softmax', \n",
    "                   kernel_regularizer=regularizers.l2(0.001))(flatten_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 22)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 22, 64)       506688      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 22, 100)      19300       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 22, 100)      25700       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 22, 100)      32100       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 22, 300)      0           ['conv1d[0][0]',                 \n",
      "                                                                  'conv1d_1[0][0]',               \n",
      "                                                                  'conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 1, 300)       0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 300)          0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 6)            1806        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 585,594\n",
      "Trainable params: 585,594\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model = Model(inputs=word_id_inputs, outputs=out)\n",
    "\n",
    "cnn_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "cnn_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• monitor (str) – Which metric to monitor in order to decay the learning rate. We will\n",
    "    monitor the validation loss <br>\n",
    "• factor (float) – By how much to reduce the learning rate. For example, a factor of 0.1\n",
    "    means that the learning rate will be reduced by 10 times (e.g. 0.01 will be stepped down\n",
    "    to 0.001) <br>\n",
    "• patience (int) – How many epochs to wait without an improvement, before reducing\n",
    "    the learning rate <br>\n",
    "• mode (string) – Whether to look for an increase or decrease of the metric; ‘auto’ means\n",
    "    that the direction will be determined by looking at the metric name <br>\n",
    "• min_delta (float) – How much of an increase/decrease to consider as an improvement <br>\n",
    "• min_lr (float) – Minimum learning rate (floor)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 02:05:15.324893: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-09-05 02:05:16.390382: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x21962d50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-05 02:05:16.390429: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 1650, Compute Capability 7.5\n",
      "2023-09-05 02:05:16.421708: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-05 02:05:16.700346: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 11s 175ms/step - loss: 1.6280 - accuracy: 0.3818 - val_loss: 1.4359 - val_accuracy: 0.5714 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "39/39 [==============================] - 5s 120ms/step - loss: 1.0994 - accuracy: 0.6863 - val_loss: 0.8518 - val_accuracy: 0.7088 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "39/39 [==============================] - 3s 75ms/step - loss: 0.6065 - accuracy: 0.8133 - val_loss: 0.5537 - val_accuracy: 0.8370 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "39/39 [==============================] - 2s 60ms/step - loss: 0.3363 - accuracy: 0.9203 - val_loss: 0.4327 - val_accuracy: 0.8626 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.1916 - accuracy: 0.9656 - val_loss: 0.3841 - val_accuracy: 0.8791 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 0.1184 - accuracy: 0.9847 - val_loss: 0.3599 - val_accuracy: 0.8901 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.0830 - accuracy: 0.9923 - val_loss: 0.3537 - val_accuracy: 0.8901 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0647 - accuracy: 0.9976 - val_loss: 0.3517 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9986 - val_loss: 0.3504 - val_accuracy: 0.8956 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0497 - accuracy: 0.9996 - val_loss: 0.3498 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.0459 - accuracy: 0.9996 - val_loss: 0.3555 - val_accuracy: 0.8883 - lr: 0.0010\n",
      "Epoch 12/25\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0429 - accuracy: 0.9996 - val_loss: 0.3523 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Epoch 13/25\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9996\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 0.0404 - accuracy: 0.9996 - val_loss: 0.3541 - val_accuracy: 0.8901 - lr: 0.0010\n",
      "Epoch 14/25\n",
      "39/39 [==============================] - 0s 13ms/step - loss: 0.0387 - accuracy: 0.9998 - val_loss: 0.3535 - val_accuracy: 0.8938 - lr: 1.0000e-04\n",
      "Epoch 15/25\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 0.0385 - accuracy: 0.9998 - val_loss: 0.3532 - val_accuracy: 0.8938 - lr: 1.0000e-04\n",
      "Epoch 16/25\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0383 - accuracy: 0.9998\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.0383 - accuracy: 0.9998 - val_loss: 0.3531 - val_accuracy: 0.8938 - lr: 1.0000e-04\n",
      "Epoch 17/25\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.0381 - accuracy: 0.9998 - val_loss: 0.3531 - val_accuracy: 0.8938 - lr: 1.0000e-05\n",
      "Epoch 18/25\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.0381 - accuracy: 0.9998 - val_loss: 0.3531 - val_accuracy: 0.8938 - lr: 1.0000e-05\n",
      "Epoch 19/25\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 0.0377 - accuracy: 1.0000\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.0381 - accuracy: 0.9998 - val_loss: 0.3531 - val_accuracy: 0.8938 - lr: 1.0000e-05\n",
      "Epoch 20/25\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0381 - accuracy: 0.9998 - val_loss: 0.3531 - val_accuracy: 0.8938 - lr: 1.0000e-06\n",
      "Epoch 21/25\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.0380 - accuracy: 0.9998 - val_loss: 0.3531 - val_accuracy: 0.8938 - lr: 1.0000e-06\n",
      "Epoch 22/25\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9998\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.0380 - accuracy: 0.9998 - val_loss: 0.3531 - val_accuracy: 0.8938 - lr: 1.0000e-06\n",
      "Epoch 23/25\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.0380 - accuracy: 0.9998 - val_loss: 0.3531 - val_accuracy: 0.8938 - lr: 1.0000e-06\n",
      "Epoch 24/25\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.0380 - accuracy: 0.9998 - val_loss: 0.3531 - val_accuracy: 0.8938 - lr: 1.0000e-06\n",
      "Epoch 25/25\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 0.0380 - accuracy: 0.9998 - val_loss: 0.3531 - val_accuracy: 0.8938 - lr: 1.0000e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6b62af0910>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_reduce_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=3, verbose=1,\n",
    "    mode='auto', min_delta=0.0001, min_lr=0.000001\n",
    ")\n",
    "\n",
    "cnn_model.fit(\n",
    "    preprocessed_train_sequences, train_labels,\n",
    "    validation_data=(preprocessed_valid_sequences, valid_labels),\n",
    "    batch_size=128,\n",
    "    epochs=25,\n",
    "    callbacks=[lr_reduce_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3636 - accuracy: 0.8880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.36357253789901733, 'accuracy': 0.8880000114440918}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.evaluate(preprocessed_test_sequences, test_labels, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
